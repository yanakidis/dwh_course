## Домашнее задание 3.
### Команда - Дмитрий Янаков, Владислав Люкшин, Данила Грашенков.

Настройка:
1) Выполняем `docker compose up airflow-init`, чтобы инициализировать Airflow.
2) Поднимаем всю систему с помощью `sh docker-init.sh`.
3) Заходим в UI Airflow по `localhost:8080` (Username - `airflow`, Password - `airflow`).
4) Добавляем подключение: Connection Id - `postgres_dwh`, Connection Type - `postgres`, Host - `host.docker.internal`, Database - `postgres`, Login - `postgres`, Password - `postgres`, Port - `5434`.
5) Триггерим даги!

После запуска дагов в DWH появится новая схема **presentation** и две новые таблицы, соответствующие двум ETL.

Сделали без бонусных заданий. 

Некоторые примечания:
1) 2й даг:
Для удобства сделаны 2 запроса:
 ⁃ 1й (закомменчен в коде) - строго выполняет то, что указано в задании, считает статистику только за прошедший день, но неудобно для дебага/тестирования, т.к. будет либо пустой выход (если не будет перелетов за вчерашний день), либо будет статистика только за вчерашний день. Поэтому сделали дополнительный 2й запрос для удобного тестирования.
 ⁃ 2й - непосредственно используется в даге, почти идентичен 1му запросу, отличие лишь в том, что статистика пересчитается для всех дней, поэтому удобнее тестировать и получаются более наглядные результаты за бОльшее кол-во дней.
2) При тестировании дагов, мы инсертили данные в мастер, но из-за того, что дебезиум асинхронный, то в кафке сообщения могли появляться не в хронолоческом порядке вставки, поэтому в некоторых хабах не успевали появляться ключи, которые используются в линках. Мы пофиксили это слипами между инсертами. Тестовые инсерты указаны в файле **test.sql**.
